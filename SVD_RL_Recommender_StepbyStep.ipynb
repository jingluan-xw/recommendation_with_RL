{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender with SVD and Reinforcement Learning\n",
    "\n",
    "* __Background:__ Consider the following situation. We want to recommend items to users. The data we have about the user and item is summarized in a matrix $X$. It has $N$ rows representing users and $M$ columns representing items. The value in the n-th row and m-th column has 3 possible values: i) 1, if the n-th user has purchases the m-th item. ii) -1, if the n-th user has seen but did not purchase the m-th item. iii) 0, if the n-th user has not seen the m-th item before. A recommender would use these known information to guess what next item each user may like to buy and recommend that item. Such a recommender is **myopic** before it cares about the immediate reward, $R_{\\mathrm{t+1}}$, i.e. when the recommender recommends an item at time step $t$, it cares about whehter the user would buy this item at time step $t+1$.\n",
    "\n",
    "\n",
    "* __Goal:__ We would like to enable a SVD recommender to care about long-term reward by taking use of reinforcement learning (RL). In other words, we want to use RL algorithm to help an SVD recommender to care about the long-term return defined as $G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ...$, where $\\gamma\\in [0,1]$ is the discounting factor. Reinforcement Learning (RL) usually contains two major parts. First, policy evaluation (prediction problem), meaning to estimate the value function for a policy. In theory, the value function is the expected value for the future return, i.e. $G_t$. Second, policy improvement (control problem), meaning to improve the policy so that its value increases. In most cases, policy improvement must be done according the policy's value function. Therefore, the first part could be done on its own without the second part, but the second part must be done together with the first part. \n",
    "\n",
    "\n",
    "* __Methods:__  \n",
    "\n",
    "    * _The myopic (greedy) recommender_ is based on [\"Truncated SVD\"](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html), which is a popular method in the category of [collaborative filtering](https://heartbeat.fritz.ai/recommender-systems-with-python-part-iii-collaborative-filtering-singular-value-decomposition-5b5dcb3f242b) recommendation. \"Collaborative filtering\" assumes that users who share similar preferences in the past will continue to like similar items in the future. \n",
    "    * The algorithm for _policy evaluation_ is \"Semi-gradient TD(0)\" described in Chapter 9 in the Book [\"Reinforcement Learning: An Introduction\" by Richard S. Sutton and Andrew G. Barto](http://incompleteideas.net/book/bookdraft2017nov5.pdf). The algorithm for _policy improvement_ will be discussed in details later in the Section on \"Policy Improvement\" in this notebook. This is a simple and naive idea of mine and I am not sure whether it would work out. I guess this simple idea probably has been investigated by other people before. I don't dive into literature for now and this notebook is not meant for publication for now. \n",
    "    * The interaction between users and the recommender would be simulated by a _Monte Carlo Simulator_.\n",
    "\n",
    "\n",
    "\n",
    "## Initialization\n",
    "\n",
    "Let us import libaries, define some parameters and initialize some variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is unavailable\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(\"cuda is available\")\n",
    "    import torch.cuda as t\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    print(\"cuda is unavailable\")\n",
    "    import torch as t\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Define the ground-truth `Xtrue` containing the true preferences of users to items. The Monte-Carlo simulator knows `Xtrue` in order to simulate the user's response to the recommender's recommendations. The recommender of course does not know `Xtrue`.\n",
    "\n",
    "* Initialize `X`, by revealing each user's preferences to `M_start` items randomly. The recommender could only see `X` which contains users' preferences to items that have been shown to them.\n",
    "\n",
    "* Define two functions: one for creating polynomial feature vector with power 2, the other is to choose an un-seen item in each row of `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- Define Parameters ----------------\n",
    "N = 1000 # number of users\n",
    "M = 100 # number of items\n",
    "K = 10 # number of singular values (eigen values) to retain in Truncated SVD.\n",
    "power = 2 # the power for polynomial feature vector, e.g. [x1, x2] --> [1, x1, x2, x1**2, x1*x2, x2**2]\n",
    "eps = 0.1 # probability for choosing the random action\n",
    "eta = 0.0 # improvement rate\n",
    "alpha = 0.3 # learning rate for Reinforcement learning v <--- (1-alpha)* v + alpha * target_v\n",
    "gamma = 0.3 # discounting factor for future return, return_t = immediate_reward + gamma * return_{t+1}\n",
    "\n",
    "# Initialize the weights in action-value function, Wa, to all zeros.\n",
    "n_S = K * (N + M + 1)\n",
    "n_SA = K * (N + M + 1) + N \n",
    "len_Ws = int(n_S**2/2 + 3*n_S/2 + 1)\n",
    "len_Wa = int(n_SA**2/2 + 3*n_SA/2 + 1)\n",
    "Ws = torch.zeros(len_Ws, device=device)\n",
    "Wa = torch.zeros(len_Wa, device=device)\n",
    "\n",
    "# `Xtrue` is the ground-truth matrix for all users' preferences to all items. \n",
    "# It is known by the Monte-Carlo Simulator but not known by the recommender.\n",
    "# `Xtrue` is generated randomly such that its element is 1 by 50% chance and \n",
    "# -1 by 50% chance.\n",
    "Xtrue = torch.rand(N, M, device=device)\n",
    "Xtrue[Xtrue >= 0.5] = 1 \n",
    "Xtrue[Xtrue < 0.5] = -1\n",
    "\n",
    "# In real life, a recommender faces a continuing task. For simplicity of this project, we \n",
    "# artificially define an \"episode\" during which the recommender interacts with users \n",
    "# by M_recom times.\n",
    "M_recom = int(0.2 * M)\n",
    "\n",
    "# We initialize X by revealing the ground-truth preferences to M_start items for each user\n",
    "M_start = int(0.1 * M)\n",
    "\n",
    "# Initialize X\n",
    "X = torch.zeros(N, M, device=device)\n",
    "X_now = X\n",
    "\n",
    "for i in range(0, N):\n",
    "    rand_index = np.random.choice(range(0,M), size=M_start, replace=False)\n",
    "    X[i, rand_index] = Xtrue[i, rand_index]\n",
    "    \n",
    "\n",
    "# A function creating polynomial feature vector of power 2.\n",
    "def poly2_features(x_vector):\n",
    "    '''\n",
    "    Map original features to its polynomial feature vector with power=2 and \n",
    "    normalize the resulting feature vector so that its L2-norm = 1, e.g.\n",
    "    [x1, x2, x3] ---> pre_vector = [1, x1, x2, x3, x1*x2, x2*x3, x1*x3, x1^2, x2^2, x3^2], \n",
    "    and then map the pre_vector to: pre_vector/(L2-norm of pre_vector)\n",
    "    \n",
    "    Input:\n",
    "    ------\n",
    "    x_vector: 2D torch tensor of shape ([1, num_columns])\n",
    "              containing the orginal features. \n",
    "    \n",
    "    Output:\n",
    "    ------\n",
    "    x_poly2: 1D torch vector of shape ([length]) \n",
    "             containing the polynomial features with power 2. \n",
    "    '''\n",
    "    x_2 = x_vector.reshape(-1,1) * x_vector\n",
    "    num_columns = x_vector.shape[1]\n",
    "    x_2 = x_2[np.triu_indices(num_columns)]\n",
    "    x_poly2_pre = torch.cat([t.FloatTensor([1]), x_vector.reshape(-1), x_2], dim=0)\n",
    "    x_poly2 = x_poly2_pre / torch.norm(x_poly2_pre)\n",
    "    \n",
    "    return x_poly2\n",
    "\n",
    "# Pick an unseen item randomly for each user (row).\n",
    "def rand_unseen(row):\n",
    "    return np.random.choice(np.argwhere(row == 0).reshape(-1), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish the Policy\n",
    "\n",
    "* __Truncated SVD:__ One way for [collaborative filtering](https://heartbeat.fritz.ai/recommender-systems-with-python-part-iii-collaborative-filtering-singular-value-decomposition-5b5dcb3f242b) recommendation is to use [\"Truncated SVD\"](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html) to decompose the matrix $X$ and reconstruct this matrix's approximation $\\tilde X$. \"Truncated\" here means that we only retain the first $K \\ll \\mathrm{min}(N, M)$ largest eigen values for $X^\\mathrm{T}X$ in the process of Singular Value Decomposition (SVD). The zeros in the original $X$ would be mostly filled with non-zero values in the approximation $\\tilde X$. These re-filled non-zeros values (which were zero before) can be regarded as the predicted \"probability\" for the user to purchase this new item. An SVD recommender would recommend the item with the highest re-filled value for each user. I see `torch` does not have TrunactedSVD yet. So I use the one in `sklearn`.\n",
    "\n",
    "\n",
    "* __$\\epsilon$-greedy policy__: The svd-action is based on the \"Truncated SVD\" briefly described above. We will use the action-value function (to be evaluated later) to make some improvement the \"Truncated SVD\" action. The improved version is the greedy action. The policy adopts the greedy action most of the time (by probability $1-\\epsilon$), and recommends a item among the un-seen items randomly for the rest of the time (by probability $\\epsilon \\ll 1$). Given that the data matrix $X$ is usually very sparse, i.e. the recommender knows only a little about each user's preference to items, the predicted \"probability\" for purchase cannot be correct all the time. So it makes sense that for a small amount of time, we just recommend random items to the users. The $\\epsilon$ part of the policy allows us to explore \"what is unknown by the recommender\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truncated SVD\n",
    "\n",
    "* Truncated SVD on `X` leading to the matrix `U`, `Sigma`, and `Vt`. \n",
    "\n",
    "* Refill (or re-construct) `X` and assign the refilled version to `X_refilled`. `X_refilled` contains predicted preferences to un-seen items. \n",
    "\n",
    "* Find the un-seen item with the max preference predicted by the Truncated SVD method for each user (row in `X_refilled`), and assign the indices of those items to `A_greedy`. `A_greedy` is named in this way meaning \"greedy action\" which exploits the known knowledge greedily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 10 * K # number of iterations for Truncated SVD.\n",
    "\n",
    "# If you need U, Sigma, Vt directly, please call the following two lines.\n",
    "from sklearn.utils.extmath import randomized_svd\n",
    "U, Sigma, Vt = randomized_svd(X.numpy(), n_components=K, n_iter=n_iter)\n",
    "X_refilled = np.matmul(np.matmul(U, np.diag(Sigma)), Vt)\n",
    "\n",
    "# Assign very negative number to items that have been seen by users.\n",
    "# So that they won't be picked by the recommender.\n",
    "X_refilled[X != 0] = -1e3\n",
    "\n",
    "# find the index of the item with the highest predicted preference for each user (row)\n",
    "A_svd = np.apply_along_axis(np.argmax, 1, X_refilled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Improve the Action (Policy Improvement) \n",
    "\n",
    "We now have found the svd action based on the Truncated SVD. We like to improve this greedy action based on the action-value function to be learned by Reinforcement Learning, $q(s,a)$.\n",
    "\n",
    "* Use `U`, `Sigma` and `Vt` to compose the state vector, `S_now`, which will be used to calculate the value functions, which is the essential part of reinforcement learning.\n",
    "\n",
    "\n",
    "* Use the approximate action-value function, $q(s, a) = W_a^\\mathrm{T} \\mathbf{x}(s,a)$, to improve the greedy action. Here $\\mathrm{x}(s,a)$ is the polynomial feature vector for the state and action, which we call `S_A_poly2` in the cell below. Note that $q$ is linear in $W_a$. The weights for $q$, $W_a$, will be iteratively learned by \"Semi-gradient TD(0)\" algorithm later. This algorithm is described in Chapter 9 in the book \"Reinforcement Learning: An Introduction\" by Sutton and Barto. I adopt the following assignment for the __policy improvement__:\n",
    "\n",
    "    $A_\\mathrm{greedy} \\leftarrow A_\\mathrm{greedy} + \\eta {\\partial q(S_\\mathrm{now}, A_\\mathrm{greedy})\\over \\partial A_\\mathrm{greedy}}\\, ,$\n",
    "    \n",
    "    where $\\eta$ is a positive number which I call \"improvement rate\".\n",
    "    \n",
    "__Note:__ Usually policy improvement means to find the action that maximizes the action-value function, meaning that $A_\\mathrm{greedy} \\leftarrow \\mathrm{argmax}_{a} q(S_\\mathrm{now}, a)$. However, this is not practical. Therefore, I choose to first find the action based on Truncated-SVD and then use the action-value function to make a small improvement on the Truncated-SVD action. It is a \"small improvement\" because $0 < \\eta <1$ (small) and this change to the action would make the $q$ a bit greater (improvement)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale Sigma\n",
    "Sigma_scaled = (Sigma - Sigma.min()) / (Sigma.max() - Sigma.min())\n",
    "\n",
    "# Represent the current state by S_now\n",
    "S_now = np.concatenate([U.reshape(-1), Sigma_scaled, Vt.reshape(-1)], axis=0)\n",
    "S_now = t.FloatTensor(S_now).requires_grad_(True)\n",
    "\n",
    "\n",
    "# I need dq/dA_greedy, so I shall define A_greedy as a torch.tensor with\n",
    "# requires_grad=True.\n",
    "A_svd = t.FloatTensor(A_svd).requires_grad_(True)\n",
    "\n",
    "# Scaled A_greedy\n",
    "A_svd_scaled = A_svd/M\n",
    "\n",
    "# Put S_now and A_greedy_scaled into a 1D torch.Tensor\n",
    "S_A = torch.cat([S_now.reshape(1,-1), A_svd_scaled.reshape(1,-1)], dim=1)\n",
    "\n",
    "# Construct a polynomial Feature vector with power 2 for the state and action pair, (S_now, A_greedy)\n",
    "S_A_poly2 = poly2_features(S_A)\n",
    "\n",
    "# Calculate action-value q(S_now, A_greedy)\n",
    "q_Asvd = torch.matmul(Wa.reshape(1,-1), S_A_poly2.reshape(-1,1)) \n",
    "\n",
    "# This could only be done once, once it is done, the graph leading to q_Agreedy is destroyed.\n",
    "q_Asvd.backward()\n",
    "\n",
    "# dq/dAgreedy, A_greedy.grad could only be called once as well!\n",
    "dq_dAsvd = A_svd.grad\n",
    "\n",
    "# Improve A_greedy, remember to make sure A_greedy contains integers\n",
    "A_greedy = torch.round(A_svd + eta * dq_dAsvd).long()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Action\n",
    "\n",
    "Recall that we decide to adopt an $\\epsilon$-greedy policy, meaning that for a small amount of time we take a random action (by probabily $\\epsilon$) in order to explore what is unknown to the recommender. We have found and improved the greedy action, `A_greedy`. Now let us randomly pick an un-seen item for each user, which is the random action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is to make sure that A_rand has the same type as A_greedy\n",
    "A_rand = t.IntTensor(np.apply_along_axis(rand_unseen, 1, X).reshape(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\epsilon$-Greedy Policy\n",
    "\n",
    "* Choose the greedy action, `A_greedy`, by the probability $1-\\epsilon$.\n",
    "\n",
    "* Choose the random action, `A_rand`, by the probability $\\epsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A_now is the current action taken for the current state S_now\n",
    "# A_now follows an epsilon-greedy policy\n",
    "\n",
    "choose_greedy = np.random.choice([True, False], 1, [1e0-eps, eps])\n",
    "# The unscaled A_now is for updating the next X, since A_now contains the indices for items to recommend.\n",
    "if choose_greedy:\n",
    "    A_now = A_greedy\n",
    "else:\n",
    "    A_now = A_rand\n",
    "    \n",
    "# Scale the action vector, A_now, this scaled version is for calculating value functions\n",
    "A_now_scaled = A_now.float() / M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design a Class \"Policy\".\n",
    "\n",
    "Now it is time to wrap up the sections on __\"Truncated SVD\", \"Improve the Action\", \"Random Action\", \"$\\epsilon$-Greedy Policy\"__ described above into a Class `Policy`. This Class should map `X` (users' preferences to part of items, visible to the recommender) to action `A` using the $\\epsilon$-greedy policy. It has the following attributes and handy methods. The symbol denoting a such a map is conventionally $\\pi : s \\rightarrow a$, mapping state to an action.\n",
    "\n",
    "* __Attributes:__ \n",
    "    * `X`: input 2D matrix containing part of users' preferences.\n",
    "    * `eps`: the value of epsilon for the $\\epsilon$-greedy policy.\n",
    "    * `eta`: improvement rate.\n",
    "    * `Wa`: the weights in the action-value function.\n",
    "    * `only_svd`: True if taking the action based on Truncated SVD, False if taking the action based on SVD improved with RL.\n",
    "    * `K`: the number of sigular values to retain in the Truncated SVD algorithm.\n",
    "    * `N`: the number of rows in `X`, equal to the number of users.\n",
    "    * `M`: the number of columns in `X`, equal to the number of items.\n",
    "    * `S`: 1D vector containing elements in the matrices, `U`, `Sigma`, `Vt`, resulting from the Truncated SVD of `X`. This 1D vector represents the current state approximately. Its length is `len(S)` $= K(N+M+1)$.\n",
    "    * `A`: 1D int vector containing the indices of items to be recommended to users, i.e. $m(n)$ for $n=0,..., N-1$. So its length is `len(A)` $= N$.\n",
    " \n",
    "\n",
    "* __Methods must be called:__\n",
    "    * .fill_S(self): fill the attribute `self.S`.\n",
    "    * .fill_A(self): fill the attribute `self.A`. Choose one from `A_greedy` and `A_rand` as the action to be taken with probabilities of $1-\\epsilon$ and $\\epsilon$ respectively.\n",
    "    \n",
    "    You must call both `.fill_S` and `.fill_A` after you define object in the Class `Policy`. Otherwise, the attributes `Policy.S` and `Policy.A` would be all zeros.\n",
    "    \n",
    "    \n",
    "* __Other Methods:__\n",
    "    * .get_U_Sigma_Vt(self): get the matrix factorizations due to Truncated SVD.\n",
    "    * .get_Asvd(self): get the recommendation vector `Asvd` based on the Truncated SVD algorithm.\n",
    "    * .get_Agreedy(self, Wa): get the improved action, `Agreedy`, based on `Asvd` and the action-value function's weights `Wa`.\n",
    "    * .get_Arand(self): get the random action, meaning randomly recommend one un-seen item to each user.\n",
    "    * .get_state_poly(self): get the polynomial feature vector representing the state.\n",
    "    * .get_state_action_poly(self): get the polynomial feature vector representing the state and the action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy():\n",
    "    '''\n",
    "    Policy, mappying state to action.\n",
    "    '''\n",
    "    def __init__(self, X, K, eps, eta, Wa, only_svd=False):\n",
    "        self.X = X\n",
    "        self.K = K\n",
    "        self.eps = eps\n",
    "        self.eta = eta\n",
    "        self.Wa = Wa\n",
    "        self.only_svd = only_svd\n",
    "        self.N = X.shape[0]\n",
    "        self.M = X.shape[1]\n",
    "        self.S = t.zeros(K * (self.N + self.M +1))\n",
    "        self.A = t.zeros(self.N)\n",
    "        \n",
    "    def get_U_Sigma_Vt(self):\n",
    "        '''\n",
    "        Get the matrix factorizations due to Truncated SVD.\n",
    "        '''\n",
    "        n_iter = 10 * self.K # number of iterations for Truncated SVD.\n",
    "\n",
    "        # Truncated SVD: X ---> U, Sigma, Vt ---> X_refilled\n",
    "        from sklearn.utils.extmath import randomized_svd\n",
    "        U, Sigma, Vt = randomized_svd(self.X.numpy(), n_components=self.K, n_iter=n_iter)\n",
    "        \n",
    "        return U, Sigma, Vt\n",
    "        \n",
    "    def get_Asvd(self):\n",
    "        '''\n",
    "        Get the action due to Truncated SVD algorithm\n",
    "        '''\n",
    "        U, Sigma, Vt = self.get_U_Sigma_Vt()\n",
    "        X_refilled = np.matmul(np.matmul(U, np.diag(Sigma)), Vt)\n",
    "\n",
    "        # Assign very negative number to items that have been seen by users.\n",
    "        # So that they won't be picked by the recommender.\n",
    "        X_refilled[self.X != 0] = -1e3\n",
    "\n",
    "        # find the index of the item with the highest predicted preference for each user (row)\n",
    "        A_svd = np.apply_along_axis(np.argmax, 1, X_refilled)\n",
    "        \n",
    "        return A_svd\n",
    "    \n",
    "    def fill_S(self):\n",
    "        '''\n",
    "        Fill the attribute self.S.\n",
    "        '''        \n",
    "        # call get_Asvd\n",
    "        U, Sigma, Vt = self.get_U_Sigma_Vt()\n",
    "        \n",
    "        # scale Sigma\n",
    "        Sigma_scaled = (Sigma - Sigma.min()) / (Sigma.max() - Sigma.min())\n",
    "\n",
    "        # Represent the current state by S_now\n",
    "        S_now = np.concatenate([U.reshape(-1), Sigma_scaled, Vt.reshape(-1)], axis=0)\n",
    "        self.S = t.FloatTensor(S_now).requires_grad_(True)\n",
    "    \n",
    "    def get_Agreedy(self):\n",
    "        '''\n",
    "        Get the greedy action, an improved version of A_svd based on the action-value function.\n",
    "        '''\n",
    "        # Call get_S\n",
    "        A_svd = self.get_Asvd()\n",
    "\n",
    "        # I need dq/dA_greedy, so I shall define A_greedy as a torch.tensor with\n",
    "        # requires_grad=True.\n",
    "        A_svd = t.FloatTensor(A_svd).requires_grad_(True)\n",
    "\n",
    "        # Scaled A_greedy\n",
    "        A_svd_scaled = A_svd / self.M\n",
    "\n",
    "        # Put S_now and A_greedy_scaled into a 1D torch.Tensor\n",
    "        S_A = torch.cat([self.S.reshape(1,-1), A_svd_scaled.reshape(1,-1)], dim=1)\n",
    "\n",
    "        # Construct a polynomial Feature vector with power 2 for the state and action pair, (S_now, A_greedy)\n",
    "        S_A_poly2 = poly2_features(S_A)\n",
    "\n",
    "        # Calculate action-value q(S_now, A_greedy)\n",
    "        q_Asvd = torch.matmul(self.Wa.reshape(1,-1), S_A_poly2.reshape(-1,1)) \n",
    "\n",
    "        # This could only be done once, once it is done, the graph leading to q_Agreedy is destroyed.\n",
    "        q_Asvd.backward()\n",
    "\n",
    "        # dq/dAgreedy, A_greedy.grad could only be called once as well!\n",
    "        dq_dAsvd = A_svd.grad\n",
    "\n",
    "        # Improve A_greedy, remember to make sure A_greedy contains integers\n",
    "        A_greedy = torch.round(A_svd + self.eta * dq_dAsvd).long()\n",
    "        \n",
    "        return A_greedy\n",
    "    \n",
    "    def get_Arand(self):\n",
    "        '''\n",
    "        Get the random action.\n",
    "        '''\n",
    "        # This is to make sure that A_rand has the same type as A_greedy\n",
    "        A_rand = t.IntTensor(np.apply_along_axis(rand_unseen, 1, self.X).reshape(-1))\n",
    "        \n",
    "        return A_rand\n",
    "    \n",
    "    def fill_A(self):\n",
    "        '''\n",
    "        Choose one action from A_greedy and A_rand by chances 1-eps and eps respectively.\n",
    "        '''\n",
    "        if self.only_svd:\n",
    "            A_svd = self.get_Asvd()\n",
    "            self.A = t.IntTensor(A_svd).reshape(-1)\n",
    "        else:\n",
    "            A_greedy = self.get_Agreedy()\n",
    "            A_rand = self.get_Arand()\n",
    "\n",
    "            choose_greedy = np.random.choice([True, False], 1, [1e0-self.eps, self.eps])\n",
    "            # The unscaled A_now is for updating the next X, \n",
    "            # since A_now contains the indices for items to recommend.\n",
    "            if choose_greedy:\n",
    "                A_now = A_greedy\n",
    "            else:\n",
    "                A_now = A_rand\n",
    "\n",
    "            # Assign the attribute\n",
    "            self.A = A_now\n",
    "    \n",
    "    def get_state_poly(self):\n",
    "        '''\n",
    "        Get the polynomial feature with power=2 for the state.\n",
    "        '''\n",
    "        S_poly2 = poly2_features(self.S.reshape(1,-1))\n",
    "        \n",
    "        return S_poly2\n",
    "    \n",
    "    def get_state_action_poly(self):\n",
    "        '''\n",
    "        Get the polynomial feature with power=2 for the (state, action) pair.\n",
    "        '''\n",
    "        A_now_scaled = self.A.float() / self.M\n",
    "        \n",
    "        # Put S_now and A_greedy_scaled into a 1D torch.Tensor\n",
    "        S_A = torch.cat([self.S.reshape(1,-1), A_now_scaled.reshape(1,-1)], dim=1)\n",
    "\n",
    "        # Construct a polynomial Feature vector with power 2 for the state and action pair\n",
    "        S_A_poly2 = poly2_features(S_A)\n",
    "        \n",
    "        return S_A_poly2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pi = Policy(X_now, K, eps, eta, Wa)\n",
    "Pi.fill_S()\n",
    "Pi.fill_A()\n",
    "\n",
    "A_now = Pi.A\n",
    "S_poly2 = Pi.get_state_poly()\n",
    "S_A_poly2 = Pi.get_state_action_poly()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observe the Next State, the Immediate Reward. \n",
    "\n",
    "* __Next state:__ Once the recommender recommends one item to each user, the users would react by \"purchasing\" (+1) or \"not purchasing\" (-1). The ground-truth user's preferences are stored in the matrix `Xtrue`. For example, the recommender recommends the m-th item to the n-th user, then we would assign the n-th row & m-th column element in `X` \n",
    "to that element in `Xtrue` at the same position. After updating `X`, we could use it to compose the next state, `S_next`.\n",
    "\n",
    "\n",
    "* __Immediate reward:__ We care about the average (across all users) user's satisfaction of our recommendations. So I define the averaged true preference to the recommended item across all users as the immediate reward, `R_next`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xtrue, Xnow, Anow ---> Xnext, Rnext\n",
    "def get_next(Xtrue, X_now, A_now):\n",
    "    '''\n",
    "    Get the next state and reward given the current state, action and ground-truth state\n",
    "    \n",
    "    Input:\n",
    "    -----\n",
    "    Xtrue: 2D int matrix, the ground-true preferences of users to items.\n",
    "    X_now: 2D int matrix, the current revealed preferences of users to items.\n",
    "    A_now: 1D int vector, the indices of items to be recommended to all users.\n",
    "    \n",
    "    Output:\n",
    "    -------\n",
    "    X_next: 2D int matrix, adding users' preferences to newly recommended items to X_now.\n",
    "    R_next: Float, the mean user's preference to the recommended items.\n",
    "    '''\n",
    "    \n",
    "    N = Xtrue.shape[0] # number of rows (users)\n",
    "    reveal_indices = (np.arange(0,N), np.array(A_now))\n",
    "    X_next = X_now\n",
    "    X_next[reveal_indices] = Xtrue[reveal_indices]\n",
    "    R_next = Xtrue[reveal_indices].sum() / N\n",
    "    return X_next, R_next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the next state matrix `X_next` and the immediate reward `R_next`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_next, R_next = get_next(Xtrue, X_now, A_now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the polynomial feature vectors for the next state and the next (state, action) pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pi_next = Policy(X_next, K, eps, eta, Wa)\n",
    "Pi_next.fill_S()\n",
    "Pi_next.fill_A()\n",
    "\n",
    "S_poly2_next = Pi_next.get_state_poly()\n",
    "S_A_poly2_next = Pi_next.get_state_action_poly()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Evaluation (Prediction Problem)\n",
    "\n",
    "I adopt the \"Semi-gradient TD(0)\", specifically Equation (9.9), in the Book [\"Reinforcement Learning: An Introduction\" by Richard S. Sutton and Andrew G. Barto](http://incompleteideas.net/book/bookdraft2017nov5.pdf). I copy this equation below.\n",
    "\n",
    "$ \\mathbf{w}_{t+1} \\leftarrow \\mathrm{w}_t + \\alpha\\left(R_{t+1} \\mathbf{x}_t - \\mathrm{x}_t(\\mathrm{x}_t-\\gamma\\mathrm{x}_{t+1})^\\mathrm{T} \\mathrm{w}_t\\right)$,\n",
    "\n",
    "where the feature vector $\\mathrm{x}_t$ in our case is the polynomial feature vector with power 2. This formula applies to both the weight vector for the state-value function\n",
    "\n",
    "$v(s) = \\mathrm{W}_s^\\mathrm{T} \\mathrm{x}(s)$ ,\n",
    "\n",
    "and the action-value function,\n",
    "\n",
    "$q(s, a) = \\mathrm{W}_a^\\mathrm{T} \\mathrm{x}(s, a)$ .\n",
    "\n",
    "When we update the weight vector `Ws`, we make the following replacements to Equation (9.9):\n",
    "\n",
    "* $\\mathrm{x}_t\\rightarrow$ `S_poly2`\n",
    "* $\\mathrm{x}_{t+1}\\rightarrow$ `S_poly2_next`\n",
    "* $\\mathrm{w}_{t+1}\\, , \\mathrm{w}_t\\rightarrow$ `Ws`.\n",
    "* $R_{t+1}\\rightarrow$ `R_next`.\n",
    "\n",
    "When we update the weight vector `Wa`, we make the following replacements to Equation (9.9):\n",
    "\n",
    "* $\\mathrm{x}_t\\rightarrow$ `S_A_poly2`\n",
    "* $\\mathrm{x}_{t+1}\\rightarrow$ `S_A_poly2_next`\n",
    "* $\\mathrm{w}_{t+1}\\, , \\mathrm{w}_t\\rightarrow$ `Wa`.\n",
    "* $R_{t+1}\\rightarrow$ `R_next`.\n",
    "\n",
    "The cell below could only be run once per time step in the Monte Carlo simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Ws, weights for the state-value function.\n",
    "Ws_Delta_S = torch.mm((S_poly2 - gamma*S_poly2_next).reshape(1,-1), Ws.reshape(-1,1)).reshape(-1)\n",
    "Ws = Ws + alpha * (R_next - Ws_Delta_S) * S_poly2\n",
    "\n",
    "# Update Wa, weights for the action-value function.\n",
    "Wa_Delta_SA = torch.mm((S_A_poly2 - gamma*S_A_poly2_next).reshape(1,-1), Wa.reshape(-1,1)).reshape(-1)\n",
    "Wa = Wa + alpha * (R_next - Wa_Delta_SA) * S_A_poly2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do not forget to update `X_now` when iterating a number of recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_now = X_next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping Up\n",
    "\n",
    "Now it is time to wrap up everything in this notebook into two python scripts:\n",
    "\n",
    "* \"workhorse\" script, named \"utils.py\", defining functions and classes.\n",
    "* \"simulation\" script, carrying out the Monte Carlo simulations on the interactions between users and the recommender. During each episode of the Monte Carlo simulation (e.g. 30 times of interactions), the state and action value functions would be learned and the policy would be improved according to the action-value function. One interaction refers to the process that the recommender recommends one item to each user and every user responds by either purchasing or not purchasing. The \"simulation.py\" script runs the episode until the total number of episodes reaches a pre-set maximum number or the weights for the value functions converge to a pre-set level."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
